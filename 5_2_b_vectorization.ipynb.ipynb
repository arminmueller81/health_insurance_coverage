{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization\n",
    "\n",
    "In this file, the tokenized and filtered sentences are vectorized via multihot-encoding and term-frequency-inverse-document-frequency (TF-IDF). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # data manipulation\n",
    "import numpy as np\n",
    "import joblib\n",
    "import os\n",
    "import random\n",
    "from ast import literal_eval # ensures that the tokenized sentences are read as a list.\n",
    "\n",
    "# for encoding with sparse matrices\n",
    "from scipy.sparse import vstack, csr_matrix\n",
    "\n",
    "# TF-IDF encoding\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python Version: 3.10.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.26.4', '2.2.3', '1.3.2')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# versions\n",
    "np.__version__, pd.__version__, joblib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a seed for reproducibility\n",
    "random.seed(421)           # Python random module\n",
    "np.random.seed(421)        # NumPy random generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set working directory\n",
    "os.chdir(\"working_directory_path\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_sen_filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[加强, 部门, 间, 工作, 协同, 全面, 对接, 社会, 救助, 经办, 服务, 各地...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[按规定, 定期, 社会, 公布, 基金, 收支, 情况, 参合, 人员, 待遇, 享受, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[事实, 无人, 抚养, 儿童, 监护人, 受, 监护人, 委托, 近亲属, 填写, 事实,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[慢性病, 种, 补偿, 名录, 呼吸系统, 慢性, 支气管炎, 肺气肿]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[市, 外, 省内, 定点, 医疗机构, 住院, 医疗, 待遇, 起付, 标准, 支付, 比...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              tokenized_sen_filtered\n",
       "0  [加强, 部门, 间, 工作, 协同, 全面, 对接, 社会, 救助, 经办, 服务, 各地...\n",
       "1  [按规定, 定期, 社会, 公布, 基金, 收支, 情况, 参合, 人员, 待遇, 享受, ...\n",
       "2  [事实, 无人, 抚养, 儿童, 监护人, 受, 监护人, 委托, 近亲属, 填写, 事实,...\n",
       "3              [慢性病, 种, 补偿, 名录, 呼吸系统, 慢性, 支气管炎, 肺气肿]\n",
       "4  [市, 外, 省内, 定点, 医疗机构, 住院, 医疗, 待遇, 起付, 标准, 支付, 比..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the tokenized sentences of the training data\n",
    "X_toksen_train = pd.read_csv('./y_broad/X_toksen_train.csv', converters={'tokenized_sen_filtered': literal_eval})\n",
    "X_toksen_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenized_sen_filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[市, 外, 转诊, 有效, 期满, 参保, 人员, 如需, 再次, 市, 外, 转诊, 应...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[第四十五, 条, 城乡居民, 医保, 执行, 统一, 基金, 财务制度, 会计制度, 基金...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[试点, 运行, 切实, 提高, 失能, 人员, 家属, 获得, 感, 幸福感]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[大力, 发挥, 网络, 报刊, 广播, 电视, 媒体, 宣传, 优势, 开设, 专版, 公...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[全省, 一体化, 平台, 建设, 方案, 力争, 全省, 医保, 信息系统, 灾备, 中心...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              tokenized_sen_filtered\n",
       "0  [市, 外, 转诊, 有效, 期满, 参保, 人员, 如需, 再次, 市, 外, 转诊, 应...\n",
       "1  [第四十五, 条, 城乡居民, 医保, 执行, 统一, 基金, 财务制度, 会计制度, 基金...\n",
       "2           [试点, 运行, 切实, 提高, 失能, 人员, 家属, 获得, 感, 幸福感]\n",
       "3  [大力, 发挥, 网络, 报刊, 广播, 电视, 媒体, 宣传, 优势, 开设, 专版, 公...\n",
       "4  [全省, 一体化, 平台, 建设, 方案, 力争, 全省, 医保, 信息系统, 灾备, 中心..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the tokenized sentences of the test data\n",
    "X_toksen_test = pd.read_csv('./y_broad/X_toksen_test.csv', converters={'tokenized_sen_filtered': literal_eval})\n",
    "X_toksen_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_index</th>\n",
       "      <th>ran200doc</th>\n",
       "      <th>sentences</th>\n",
       "      <th>ran20sen</th>\n",
       "      <th>sen_index</th>\n",
       "      <th>tokenized_sen</th>\n",
       "      <th>tokenized_sen_filtered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>《中外合资、合作医疗机构管理暂行办法》的补充规定        中华...</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>[' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ...</td>\n",
       "      <td>[中外合资, 合作医疗, 机构, 管理, 暂行办法, 补充规定, 中华人民共和国, 卫生部,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>卫生部部长：陈竺商务部部长：陈德铭二○○七年十二月三十日　　《中外合资、合作医疗机构管理暂行...</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>['卫生部', '部长', '：', '陈竺', '商务部', '部长', '：', '陈德...</td>\n",
       "      <td>[卫生部, 部长, 陈竺, 商务部, 部长, 陈德铭, ○, ○, 七年, 十二月, 三十日...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>二、本规定中香港、澳门服务提供者应分别符合《内地与香港关于建立更紧密经贸关系的安排》及《...</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>['\\u3000', '\\u3000', '二', '、', '本', '规定', '中',...</td>\n",
       "      <td>[规定, 香港, 澳门, 服务提供者, 应, 符合, 内地, 香港, 建立, 紧密, 经贸关...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>三、香港、澳门服务提供者在内地设立合资、合作医疗机构的其他规定，仍参照《中外合资、合作医...</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>['\\u3000', '\\u3000', '三', '、', '香港', '、', '澳门'...</td>\n",
       "      <td>[香港, 澳门, 服务提供者, 内地, 设立, 合资, 合作医疗, 机构, 规定, 参照, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>137</td>\n",
       "      <td>四、本规定自2008年1月1日起施行</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>['\\u3000', '\\u3000', '四', '、', '本', '规定', '自',...</td>\n",
       "      <td>[规定, 日起, 施行]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993521</th>\n",
       "      <td>31138</td>\n",
       "      <td>35</td>\n",
       "      <td>否则不予报销</td>\n",
       "      <td>7</td>\n",
       "      <td>1003131</td>\n",
       "      <td>['否则', '不予', '报销']</td>\n",
       "      <td>[不予, 报销]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993522</th>\n",
       "      <td>31138</td>\n",
       "      <td>35</td>\n",
       "      <td>第四章　附则　　第二十四条　本技术方案从二00八年六月一日起统一执行，同时原制定的补偿方案作废</td>\n",
       "      <td>8</td>\n",
       "      <td>1003132</td>\n",
       "      <td>['\\u3000', '第四章', '\\u3000', '附则', '\\u3000', '\\...</td>\n",
       "      <td>[第四章, 附则, 第二十四条, 技术, 方案, 八年, 六月, 一日, 统一, 执行, 原...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993523</th>\n",
       "      <td>31138</td>\n",
       "      <td>35</td>\n",
       "      <td>第二十五条　本方案由龙胜各族自治县新型农村合作医疗管理办公室负责解释</td>\n",
       "      <td>9</td>\n",
       "      <td>1003133</td>\n",
       "      <td>['\\u3000', '\\u3000', '第二十五条', '\\u3000', '本', '...</td>\n",
       "      <td>[第二十五条, 方案, 龙胜各族自治县, 新型农村, 合作医疗, 管理, 办公室, 负责, 解释]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993524</th>\n",
       "      <td>31139</td>\n",
       "      <td>91</td>\n",
       "      <td>龙胜各族自治县人民政府关于成立自治县“健康扶贫·医疗救助”公益基金管理工作领导小组的通知龙胜...</td>\n",
       "      <td>14</td>\n",
       "      <td>1003135</td>\n",
       "      <td>['龙胜各族自治县', '人民政府', '关于', '成立', '自治县', '“', '健...</td>\n",
       "      <td>[龙胜各族自治县, 人民政府, 成立, 自治县, 健康, 扶贫, ·, 医疗, 救助, 公益...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993525</th>\n",
       "      <td>31139</td>\n",
       "      <td>91</td>\n",
       "      <td>龙胜各族自治县人民政府办公室2017年8月24日</td>\n",
       "      <td>3</td>\n",
       "      <td>1003136</td>\n",
       "      <td>['龙胜各族自治县', '人民政府', '办公室', '2017', '年', '8', '...</td>\n",
       "      <td>[龙胜各族自治县, 人民政府, 办公室]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>993526 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        doc_index  ran200doc  \\\n",
       "0               1        137   \n",
       "1               1        137   \n",
       "2               1        137   \n",
       "3               1        137   \n",
       "4               1        137   \n",
       "...           ...        ...   \n",
       "993521      31138         35   \n",
       "993522      31138         35   \n",
       "993523      31138         35   \n",
       "993524      31139         91   \n",
       "993525      31139         91   \n",
       "\n",
       "                                                sentences  ran20sen  \\\n",
       "0                   《中外合资、合作医疗机构管理暂行办法》的补充规定        中华...        16   \n",
       "1       卫生部部长：陈竺商务部部长：陈德铭二○○七年十二月三十日　　《中外合资、合作医疗机构管理暂行...        18   \n",
       "2       　　二、本规定中香港、澳门服务提供者应分别符合《内地与香港关于建立更紧密经贸关系的安排》及《...        18   \n",
       "3       　　三、香港、澳门服务提供者在内地设立合资、合作医疗机构的其他规定，仍参照《中外合资、合作医...         7   \n",
       "4                                    　　四、本规定自2008年1月1日起施行        16   \n",
       "...                                                   ...       ...   \n",
       "993521                                             否则不予报销         7   \n",
       "993522   　第四章　附则　　第二十四条　本技术方案从二00八年六月一日起统一执行，同时原制定的补偿方案作废         8   \n",
       "993523               　　第二十五条　本方案由龙胜各族自治县新型农村合作医疗管理办公室负责解释         9   \n",
       "993524  龙胜各族自治县人民政府关于成立自治县“健康扶贫·医疗救助”公益基金管理工作领导小组的通知龙胜...        14   \n",
       "993525                           龙胜各族自治县人民政府办公室2017年8月24日         3   \n",
       "\n",
       "        sen_index                                      tokenized_sen  \\\n",
       "0               1  [' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ...   \n",
       "1               2  ['卫生部', '部长', '：', '陈竺', '商务部', '部长', '：', '陈德...   \n",
       "2               3  ['\\u3000', '\\u3000', '二', '、', '本', '规定', '中',...   \n",
       "3               4  ['\\u3000', '\\u3000', '三', '、', '香港', '、', '澳门'...   \n",
       "4               5  ['\\u3000', '\\u3000', '四', '、', '本', '规定', '自',...   \n",
       "...           ...                                                ...   \n",
       "993521    1003131                                 ['否则', '不予', '报销']   \n",
       "993522    1003132  ['\\u3000', '第四章', '\\u3000', '附则', '\\u3000', '\\...   \n",
       "993523    1003133  ['\\u3000', '\\u3000', '第二十五条', '\\u3000', '本', '...   \n",
       "993524    1003135  ['龙胜各族自治县', '人民政府', '关于', '成立', '自治县', '“', '健...   \n",
       "993525    1003136  ['龙胜各族自治县', '人民政府', '办公室', '2017', '年', '8', '...   \n",
       "\n",
       "                                   tokenized_sen_filtered  \n",
       "0       [中外合资, 合作医疗, 机构, 管理, 暂行办法, 补充规定, 中华人民共和国, 卫生部,...  \n",
       "1       [卫生部, 部长, 陈竺, 商务部, 部长, 陈德铭, ○, ○, 七年, 十二月, 三十日...  \n",
       "2       [规定, 香港, 澳门, 服务提供者, 应, 符合, 内地, 香港, 建立, 紧密, 经贸关...  \n",
       "3       [香港, 澳门, 服务提供者, 内地, 设立, 合资, 合作医疗, 机构, 规定, 参照, ...  \n",
       "4                                            [规定, 日起, 施行]  \n",
       "...                                                   ...  \n",
       "993521                                           [不予, 报销]  \n",
       "993522  [第四章, 附则, 第二十四条, 技术, 方案, 八年, 六月, 一日, 统一, 执行, 原...  \n",
       "993523  [第二十五条, 方案, 龙胜各族自治县, 新型农村, 合作医疗, 管理, 办公室, 负责, 解释]  \n",
       "993524  [龙胜各族自治县, 人民政府, 成立, 自治县, 健康, 扶贫, ·, 医疗, 救助, 公益...  \n",
       "993525                               [龙胜各族自治县, 人民政府, 办公室]  \n",
       "\n",
       "[993526 rows x 7 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load unlabelled data\n",
    "data_unlabelled = pd.read_csv('./data_unlabelled_tok_fil.csv', converters={'tokenized_sen_filtered': literal_eval})\n",
    "data_unlabelled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Multihot Encoding\n",
    "\n",
    "#### 2.1 Prepare vocabulary and index\n",
    "\n",
    "Multihot encoding creates a dummy variable for each word in the corpus.\n",
    "The first step is to create a vocabulary from the training data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: create a list of all tokens, looping over the sentences and the tokens in the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "330751"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of all tokens\n",
    "all_tokens = [token for sentence in X_toksen_train['tokenized_sen_filtered'] for token in sentence]\n",
    "len(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['例', '内科', '治疗', '翼状', '胬肉', '单侧', '胬肉', '切除', '手术', '例', '手术', '治疗', '翼状', '胬肉', '双侧', '胬肉', '切除', '手术', '手术', '治疗']\n"
     ]
    }
   ],
   "source": [
    "# check observations\n",
    "print(all_tokens[20000:20020])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18559"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create vocabulary through set\n",
    "vocabulary = list(set(all_tokens))\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vocabulary consists of 18,559 tokens. We save it for later to name the features in the multihot datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save vocabulary for later data\n",
    "pd.DataFrame(vocabulary).to_csv('vocabulary.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recover vocabulary, get column and turn to list\n",
    "vocabulary = pd.read_csv('vocabulary.csv').iloc[:,1].tolist()\n",
    "type(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['物价水平', 'X光', '实事', '从重处理', '分装', '跑腿', '重在', '考量', '支撑', '会商']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check vocabulary entries\n",
    "vocabulary[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data type\n",
    "type(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a dictionary, which includes the vocabulary and an index that enumerates the vocabulary, so that each word is linked to a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary to map of tokens to indices\n",
    "token_to_index = {token: index for index, token in enumerate(vocabulary)}\n",
    "type(token_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'入市': 0,\n",
       " '助学金': 1,\n",
       " '停保': 2,\n",
       " '征询': 3,\n",
       " '二十五年': 4,\n",
       " '党校': 5,\n",
       " '及下': 6,\n",
       " '送市': 7,\n",
       " '氢溴酸': 8,\n",
       " '中轴': 9,\n",
       " '盼': 10,\n",
       " '气管': 11,\n",
       " '刑事案件': 12,\n",
       " '未戴': 13,\n",
       " '免收': 14,\n",
       " '杜舟': 15,\n",
       " '事中': 16,\n",
       " '名非': 17,\n",
       " '聚集': 18,\n",
       " '土地': 19,\n",
       " '四中全会': 20,\n",
       " '正位': 21,\n",
       " '中重': 22,\n",
       " '坚固': 23,\n",
       " '短缺': 24,\n",
       " '第四十条': 25,\n",
       " '承办人': 26,\n",
       " '医疗卫生': 27,\n",
       " '实质性': 28,\n",
       " '规范': 29,\n",
       " '变通': 30,\n",
       " '肇庆市': 31,\n",
       " '问答': 32,\n",
       " '生活用品': 33,\n",
       " '高港区': 34,\n",
       " '20%': 35,\n",
       " '列账': 36,\n",
       " '翻': 37,\n",
       " '五中': 38,\n",
       " '雇佣': 39,\n",
       " '骶': 40,\n",
       " '周秋明': 41,\n",
       " '镇级': 42,\n",
       " '蝶呤': 43,\n",
       " '政协提案': 44,\n",
       " '无望': 45,\n",
       " '即可': 46,\n",
       " '挪用': 47,\n",
       " '湖里区': 48,\n",
       " 'Ｍ': 49,\n",
       " '托管': 50,\n",
       " '申请理由': 51,\n",
       " '保险': 52,\n",
       " '治疗仪': 53,\n",
       " '告示': 54,\n",
       " '85.5%': 55,\n",
       " '二年': 56,\n",
       " '纳入': 57,\n",
       " '维平': 58,\n",
       " '仍为': 59,\n",
       " '机关': 60,\n",
       " '七月': 61,\n",
       " '传播': 62,\n",
       " '李沧区': 63,\n",
       " '沿用': 64,\n",
       " '工作汇报': 65,\n",
       " '慈善机构': 66,\n",
       " '特快专递': 67,\n",
       " '异常': 68,\n",
       " '恶劣': 69,\n",
       " '内审': 70,\n",
       " '全年': 71,\n",
       " '姜': 72,\n",
       " '采样': 73,\n",
       " '榭': 74,\n",
       " '预支': 75,\n",
       " '指非': 76,\n",
       " '销售费用': 77,\n",
       " '自信': 78,\n",
       " '幼儿园': 79,\n",
       " '脑科': 80,\n",
       " '回扣': 81,\n",
       " '四肢': 82,\n",
       " '机等': 83,\n",
       " '糖苷酶': 84,\n",
       " '选项': 85,\n",
       " '时到': 86,\n",
       " '丹参': 87,\n",
       " '情况': 88,\n",
       " '三日': 89,\n",
       " '腹水': 90,\n",
       " '信委': 91,\n",
       " '条例': 92,\n",
       " '李宏': 93,\n",
       " '甘南藏族自治州': 94,\n",
       " '相继': 95,\n",
       " '愿签': 96,\n",
       " '都江堰': 97,\n",
       " '杭劳': 98,\n",
       " '退保': 99,\n",
       " '物价水平': 100,\n",
       " 'X光': 101,\n",
       " '实事': 102,\n",
       " '从重处理': 103,\n",
       " '分装': 104,\n",
       " '跑腿': 105,\n",
       " '重在': 106,\n",
       " '考量': 107,\n",
       " '支撑': 108,\n",
       " '会商': 109,\n",
       " '叶海琴': 110,\n",
       " '监管': 111,\n",
       " '虹膜': 112,\n",
       " '带给': 113,\n",
       " '债权债务': 114,\n",
       " '月市': 115,\n",
       " '分期付款': 116,\n",
       " '类型': 117,\n",
       " '社会各界': 118,\n",
       " '联发': 119,\n",
       " '反对': 120,\n",
       " '专业性': 121,\n",
       " '境': 122,\n",
       " '信息流': 123,\n",
       " '脱臼': 124,\n",
       " '剃须': 125,\n",
       " '厂': 126,\n",
       " '服务项目': 127,\n",
       " '不容忽视': 128,\n",
       " '女年满': 129,\n",
       " '座': 130,\n",
       " '珠人': 131,\n",
       " '不详': 132,\n",
       " '显微镜': 133,\n",
       " '控释': 134,\n",
       " '∶': 135,\n",
       " '征纳': 136,\n",
       " '有误': 137,\n",
       " '场馆': 138,\n",
       " '耳鼻喉科': 139,\n",
       " '精神鼓励': 140,\n",
       " '桃': 141,\n",
       " '以案释法': 142,\n",
       " '病防治': 143,\n",
       " '远场': 144,\n",
       " '或卡': 145,\n",
       " '随之': 146,\n",
       " '多动': 147,\n",
       " '防范': 148,\n",
       " '巡诊': 149,\n",
       " '漱口': 150,\n",
       " '用途': 151,\n",
       " '挂床': 152,\n",
       " '审管': 153,\n",
       " '开式': 154,\n",
       " '按预': 155,\n",
       " '科学院': 156,\n",
       " '优酬': 157,\n",
       " '难产': 158,\n",
       " '放入': 159,\n",
       " '莱西市': 160,\n",
       " '扣押': 161,\n",
       " '尹莉': 162,\n",
       " '普洱': 163,\n",
       " '控费': 164,\n",
       " '麻黄素': 165,\n",
       " '大清': 166,\n",
       " '代理': 167,\n",
       " '胬肉': 168,\n",
       " '上端': 169,\n",
       " '泗': 170,\n",
       " '对照表': 171,\n",
       " '知母': 172,\n",
       " '扬州市': 173,\n",
       " '点值': 174,\n",
       " '适配': 175,\n",
       " '达州': 176,\n",
       " '损益': 177,\n",
       " '市委': 178,\n",
       " '第三节': 179,\n",
       " '局应于': 180,\n",
       " '拨款': 181,\n",
       " '必问效': 182,\n",
       " '大夫': 183,\n",
       " '城北区': 184,\n",
       " '高级别': 185,\n",
       " '正禾府': 186,\n",
       " '福利彩票': 187,\n",
       " '按计划': 188,\n",
       " '查证': 189,\n",
       " '城乡居民': 190,\n",
       " '骨性': 191,\n",
       " '技术规范': 192,\n",
       " '熟练掌握': 193,\n",
       " '湖区': 194,\n",
       " '荆杏': 195,\n",
       " '对乙酰氨基酚': 196,\n",
       " '第三十六条': 197,\n",
       " '明码': 198,\n",
       " '轻柔': 199,\n",
       " '预防性': 200,\n",
       " '立项': 201,\n",
       " '并行': 202,\n",
       " '为例': 203,\n",
       " '虚增': 204,\n",
       " '张榜': 205,\n",
       " '市场需求': 206,\n",
       " '选本': 207,\n",
       " '湖': 208,\n",
       " '文体局': 209,\n",
       " '组合': 210,\n",
       " '领先': 211,\n",
       " '巷': 212,\n",
       " '顶格': 213,\n",
       " '小型': 214,\n",
       " '措施': 215,\n",
       " '并于': 216,\n",
       " '萎缩': 217,\n",
       " '总公司': 218,\n",
       " '或院': 219,\n",
       " '化学治疗': 220,\n",
       " '破解': 221,\n",
       " '补充': 222,\n",
       " '依法治理': 223,\n",
       " '嫌疑': 224,\n",
       " '草': 225,\n",
       " '个体差异': 226,\n",
       " '体育': 227,\n",
       " '前路': 228,\n",
       " '总支出': 229,\n",
       " '号局': 230,\n",
       " '南开': 231,\n",
       " '马巷': 232,\n",
       " '否本': 233,\n",
       " '乙': 234,\n",
       " '引流术': 235,\n",
       " '寿命': 236,\n",
       " '非本': 237,\n",
       " '民政部': 238,\n",
       " '骶骨': 239,\n",
       " '锡': 240,\n",
       " '立功受奖': 241,\n",
       " '牛': 242,\n",
       " '振达路': 243,\n",
       " '信号': 244,\n",
       " '肢体': 245,\n",
       " '失调': 246,\n",
       " '统一行动': 247,\n",
       " '中如': 248,\n",
       " '供需双方': 249,\n",
       " '特钠': 250,\n",
       " '第十一条': 251,\n",
       " '社工': 252,\n",
       " '切入点': 253,\n",
       " '防止出现': 254,\n",
       " '刘伟': 255,\n",
       " '扁桃体': 256,\n",
       " '新药': 257,\n",
       " '抵缴': 258,\n",
       " '营业人员': 259,\n",
       " '含企': 260,\n",
       " '已有': 261,\n",
       " '强调': 262,\n",
       " '抑': 263,\n",
       " '表明': 264,\n",
       " '第三章': 265,\n",
       " '思想顾虑': 266,\n",
       " '网络管理': 267,\n",
       " '农医办': 268,\n",
       " '躯体': 269,\n",
       " '总部': 270,\n",
       " '护士': 271,\n",
       " '友谊': 272,\n",
       " '任务': 273,\n",
       " '结节': 274,\n",
       " '卫健局': 275,\n",
       " '经销': 276,\n",
       " '从事': 277,\n",
       " '区别': 278,\n",
       " '乙酰': 279,\n",
       " '休克': 280,\n",
       " '盘活': 281,\n",
       " '三十六': 282,\n",
       " '决策支持系统': 283,\n",
       " '创造条件': 284,\n",
       " '资讯': 285,\n",
       " '订单': 286,\n",
       " '开立': 287,\n",
       " '近期': 288,\n",
       " '打孔': 289,\n",
       " '病期': 290,\n",
       " '上门': 291,\n",
       " '光凝法': 292,\n",
       " '紧迫性': 293,\n",
       " '逐个': 294,\n",
       " '神湾': 295,\n",
       " '分轨': 296,\n",
       " '税务系统': 297,\n",
       " '醒目': 298,\n",
       " '经营性': 299,\n",
       " '权责': 300,\n",
       " '投靠': 301,\n",
       " '包户': 302,\n",
       " '期满': 303,\n",
       " '鄂伦春自治旗': 304,\n",
       " '肝硬化': 305,\n",
       " '埋': 306,\n",
       " '六年': 307,\n",
       " '疑难病': 308,\n",
       " '零售': 309,\n",
       " '医嘱': 310,\n",
       " '正副': 311,\n",
       " '推向': 312,\n",
       " '待转': 313,\n",
       " '民德': 314,\n",
       " '转运': 315,\n",
       " '目': 316,\n",
       " '大力推广': 317,\n",
       " '私自': 318,\n",
       " '相当': 319,\n",
       " '使用量': 320,\n",
       " '资料齐全': 321,\n",
       " '监督制约': 322,\n",
       " '暖': 323,\n",
       " '一岗双责': 324,\n",
       " '所述': 325,\n",
       " '提肌': 326,\n",
       " '继发性': 327,\n",
       " '片次': 328,\n",
       " '发放': 329,\n",
       " '也就是说': 330,\n",
       " '设有': 331,\n",
       " '充分发挥': 332,\n",
       " '瓶装': 333,\n",
       " '乳化': 334,\n",
       " '扣款': 335,\n",
       " '新材': 336,\n",
       " '肇祸': 337,\n",
       " '太平洋': 338,\n",
       " '玉树': 339,\n",
       " '75%': 340,\n",
       " '新疆': 341,\n",
       " '便于管理': 342,\n",
       " '险': 343,\n",
       " '跨新': 344,\n",
       " '生得安': 345,\n",
       " '古交市': 346,\n",
       " '大连市': 347,\n",
       " '塑形': 348,\n",
       " '分立': 349,\n",
       " '费及': 350,\n",
       " '市人社': 351,\n",
       " '黄道': 352,\n",
       " '附赠': 353,\n",
       " '普法': 354,\n",
       " '福利金': 355,\n",
       " '新兴路': 356,\n",
       " '高频': 357,\n",
       " '样本': 358,\n",
       " '目的意义': 359,\n",
       " '免疫学': 360,\n",
       " '省级机关': 361,\n",
       " '四届': 362,\n",
       " '一概': 363,\n",
       " '改革方案': 364,\n",
       " '讲': 365,\n",
       " '不愈': 366,\n",
       " '新型农村': 367,\n",
       " '纯收入': 368,\n",
       " '重用': 369,\n",
       " '博明': 370,\n",
       " '胶原': 371,\n",
       " '争当': 372,\n",
       " '撰写': 373,\n",
       " '优待': 374,\n",
       " '国谈药': 375,\n",
       " '机': 376,\n",
       " '呈': 377,\n",
       " '工龄': 378,\n",
       " '亏空': 379,\n",
       " '二等功': 380,\n",
       " '报州': 381,\n",
       " '透析': 382,\n",
       " '补充协议': 383,\n",
       " '窦房结': 384,\n",
       " '两种': 385,\n",
       " '重叠': 386,\n",
       " '二天': 387,\n",
       " '鲁卫基': 388,\n",
       " '收齐': 389,\n",
       " '工疗': 390,\n",
       " '移民': 391,\n",
       " '不出': 392,\n",
       " '外显子': 393,\n",
       " '厦': 394,\n",
       " '末位': 395,\n",
       " '创业园': 396,\n",
       " '体积': 397,\n",
       " '雅安': 398,\n",
       " '影像学': 399,\n",
       " '停止使用': 400,\n",
       " '商谈': 401,\n",
       " '暂未': 402,\n",
       " '联合会': 403,\n",
       " '武清区': 404,\n",
       " '农经站': 405,\n",
       " '其当次': 406,\n",
       " '征用': 407,\n",
       " '同口径': 408,\n",
       " '业态': 409,\n",
       " '可向市': 410,\n",
       " '开办': 411,\n",
       " '曹昌银': 412,\n",
       " '入路': 413,\n",
       " '王仁元': 414,\n",
       " '内患': 415,\n",
       " '众生': 416,\n",
       " '0.6%': 417,\n",
       " '无回': 418,\n",
       " '不力': 419,\n",
       " '外分泌腺': 420,\n",
       " '诊后': 421,\n",
       " '三届': 422,\n",
       " '每组': 423,\n",
       " '夏枯草': 424,\n",
       " '大排查': 425,\n",
       " '配好': 426,\n",
       " '药制剂': 427,\n",
       " '知识库': 428,\n",
       " '自然资源': 429,\n",
       " '所称': 430,\n",
       " '粘弹': 431,\n",
       " '鄂卫通': 432,\n",
       " '议事': 433,\n",
       " '十三届': 434,\n",
       " '恶性肿瘤': 435,\n",
       " '三明市': 436,\n",
       " '跌倒': 437,\n",
       " '分明': 438,\n",
       " '考试合格': 439,\n",
       " '过低': 440,\n",
       " '局': 441,\n",
       " '依那普利': 442,\n",
       " '可回收': 443,\n",
       " '递补': 444,\n",
       " '制品': 445,\n",
       " '组织部': 446,\n",
       " '宣传栏': 447,\n",
       " '复兴': 448,\n",
       " '无异议': 449,\n",
       " '需有': 450,\n",
       " '移植': 451,\n",
       " '辅助性': 452,\n",
       " '公示': 453,\n",
       " '改动': 454,\n",
       " '瑞金医院': 455,\n",
       " '直属机关': 456,\n",
       " '三人': 457,\n",
       " '周至县': 458,\n",
       " '省': 459,\n",
       " '补正': 460,\n",
       " '国丹': 461,\n",
       " '或伴': 462,\n",
       " '其原': 463,\n",
       " '附加费': 464,\n",
       " '昆山市': 465,\n",
       " '欠佳': 466,\n",
       " '私设': 467,\n",
       " '自付': 468,\n",
       " '核酸': 469,\n",
       " '检证': 470,\n",
       " '无理取闹': 471,\n",
       " '可用': 472,\n",
       " '三七': 473,\n",
       " '改良': 474,\n",
       " '存储': 475,\n",
       " '扩张器': 476,\n",
       " '失业': 477,\n",
       " '下关': 478,\n",
       " '嘉善县': 479,\n",
       " '投诉率': 480,\n",
       " '鼻炎': 481,\n",
       " '完全': 482,\n",
       " '卫生厅': 483,\n",
       " '经胸': 484,\n",
       " '保险资金': 485,\n",
       " '变动': 486,\n",
       " '泵': 487,\n",
       " '出据': 488,\n",
       " '诊疗所': 489,\n",
       " '尽保': 490,\n",
       " '减免': 491,\n",
       " '糊丸': 492,\n",
       " '增大': 493,\n",
       " '相片': 494,\n",
       " '流性': 495,\n",
       " '最易': 496,\n",
       " '轮播': 497,\n",
       " '中本': 498,\n",
       " '渝中区': 499,\n",
       " '类药物': 500,\n",
       " '国资': 501,\n",
       " '自用': 502,\n",
       " '政策措施': 503,\n",
       " '压迫': 504,\n",
       " '值守': 505,\n",
       " '上海市公安局': 506,\n",
       " '进社区': 507,\n",
       " '线下': 508,\n",
       " '嘉奖': 509,\n",
       " 'C座': 510,\n",
       " '回': 511,\n",
       " '总计': 512,\n",
       " '以经': 513,\n",
       " '配取': 514,\n",
       " '肺癌': 515,\n",
       " '张驰': 516,\n",
       " '防范措施': 517,\n",
       " '万燕': 518,\n",
       " '外网': 519,\n",
       " '巡察': 520,\n",
       " '第五十': 521,\n",
       " '国资局': 522,\n",
       " '质量保证': 523,\n",
       " '驻兰': 524,\n",
       " '从安监': 525,\n",
       " '开展业务': 526,\n",
       " 'Ⅲ': 527,\n",
       " '金融机构': 528,\n",
       " '无需': 529,\n",
       " '后次': 530,\n",
       " '术后': 531,\n",
       " '软组织': 532,\n",
       " '区外': 533,\n",
       " '多发性': 534,\n",
       " '肾功能': 535,\n",
       " '磷脂酶': 536,\n",
       " '管理部': 537,\n",
       " '祥和': 538,\n",
       " '确切': 539,\n",
       " '试行期': 540,\n",
       " '鼻窦炎': 541,\n",
       " '诈骗': 542,\n",
       " '塘区北': 543,\n",
       " '%': 544,\n",
       " '磷脂': 545,\n",
       " '产生': 546,\n",
       " '奖': 547,\n",
       " '串': 548,\n",
       " '既要': 549,\n",
       " '建设性': 550,\n",
       " '苏州工业园区': 551,\n",
       " '标准线': 552,\n",
       " '内戴入': 553,\n",
       " '心脏起搏器': 554,\n",
       " '厅局': 555,\n",
       " '日向': 556,\n",
       " '药中': 557,\n",
       " '赋值': 558,\n",
       " '在职人员': 559,\n",
       " '全文': 560,\n",
       " '釆': 561,\n",
       " '人群': 562,\n",
       " '植骨术': 563,\n",
       " '黑龙江省政府': 564,\n",
       " '明细单': 565,\n",
       " '随同': 566,\n",
       " '加入': 567,\n",
       " '公务': 568,\n",
       " '联体': 569,\n",
       " '新视界': 570,\n",
       " '对持': 571,\n",
       " '试行': 572,\n",
       " '负责同志': 573,\n",
       " '伊犁哈萨克自治州': 574,\n",
       " '诊疗': 575,\n",
       " '更优': 576,\n",
       " '剩余时间': 577,\n",
       " '接触': 578,\n",
       " '网格': 579,\n",
       " '院区': 580,\n",
       " '黔': 581,\n",
       " '化德县': 582,\n",
       " '池次': 583,\n",
       " '泸县': 584,\n",
       " '回收': 585,\n",
       " '严守': 586,\n",
       " '囊': 587,\n",
       " '华烨': 588,\n",
       " '止血带': 589,\n",
       " '第三十七条': 590,\n",
       " '列为': 591,\n",
       " '确认': 592,\n",
       " '体检': 593,\n",
       " '泌尿系统': 594,\n",
       " '合证': 595,\n",
       " '下旬': 596,\n",
       " '肠系膜': 597,\n",
       " '教指委': 598,\n",
       " '提高质量': 599,\n",
       " '征税': 600,\n",
       " '复字': 601,\n",
       " '医治': 602,\n",
       " '认识': 603,\n",
       " '支具': 604,\n",
       " '牙性': 605,\n",
       " '沙溪隆': 606,\n",
       " '途中': 607,\n",
       " '六次': 608,\n",
       " '毛胜寺': 609,\n",
       " '颤抖': 610,\n",
       " '上杭县': 611,\n",
       " '承办': 612,\n",
       " '辽事通': 613,\n",
       " '自谋职业': 614,\n",
       " '二十一天': 615,\n",
       " '一案': 616,\n",
       " '已均': 617,\n",
       " '问题解答': 618,\n",
       " '工程': 619,\n",
       " '农民收入': 620,\n",
       " '复员军人': 621,\n",
       " '科技厅': 622,\n",
       " '指法': 623,\n",
       " '农商': 624,\n",
       " '流行': 625,\n",
       " '在思想上': 626,\n",
       " '同防': 627,\n",
       " '明码标价': 628,\n",
       " '济困': 629,\n",
       " '张勇': 630,\n",
       " '阴影': 631,\n",
       " '埋术': 632,\n",
       " '全科': 633,\n",
       " '爱尔奥理德': 634,\n",
       " '应由其': 635,\n",
       " '参试': 636,\n",
       " '较小值': 637,\n",
       " '赵娟娟': 638,\n",
       " '市值': 639,\n",
       " '强度': 640,\n",
       " '澄': 641,\n",
       " '解疑释惑': 642,\n",
       " '信息库': 643,\n",
       " '市州': 644,\n",
       " '降糖药': 645,\n",
       " '彩电': 646,\n",
       " '龙政任': 647,\n",
       " 'β': 648,\n",
       " '介绍信': 649,\n",
       " '潜能': 650,\n",
       " '危及': 651,\n",
       " '日内持': 652,\n",
       " '立体': 653,\n",
       " '制备': 654,\n",
       " '仲裁': 655,\n",
       " '行之有效': 656,\n",
       " '重点': 657,\n",
       " '椎弓': 658,\n",
       " '城市': 659,\n",
       " '痛苦': 660,\n",
       " '存疑': 661,\n",
       " '脊椎': 662,\n",
       " '运转': 663,\n",
       " '二十四日': 664,\n",
       " '教唆': 665,\n",
       " '线路': 666,\n",
       " '引领': 667,\n",
       " '王': 668,\n",
       " '唐': 669,\n",
       " '十二届': 670,\n",
       " '指导思想': 671,\n",
       " '大红鹰': 672,\n",
       " '享乐主义': 673,\n",
       " '行政级别': 674,\n",
       " '结膜': 675,\n",
       " '解答': 676,\n",
       " '轻微': 677,\n",
       " '此表': 678,\n",
       " '违法违纪': 679,\n",
       " '带队': 680,\n",
       " '刘启纯': 681,\n",
       " '上个月': 682,\n",
       " '竞标': 683,\n",
       " '后方': 684,\n",
       " '民族': 685,\n",
       " '睡眠': 686,\n",
       " '被服': 687,\n",
       " '重挂': 688,\n",
       " '城乡之间': 689,\n",
       " '电脑': 690,\n",
       " '种植义齿': 691,\n",
       " '三屯': 692,\n",
       " '己酸': 693,\n",
       " '沪卫': 694,\n",
       " 'Ⅰ': 695,\n",
       " '辽药': 696,\n",
       " '切实': 697,\n",
       " '裂': 698,\n",
       " '或遇': 699,\n",
       " '贵州': 700,\n",
       " '金属': 701,\n",
       " '局内': 702,\n",
       " '选': 703,\n",
       " '连云港市': 704,\n",
       " '违法行为': 705,\n",
       " '加挂': 706,\n",
       " '黄玲': 707,\n",
       " '镇静': 708,\n",
       " '厦门市': 709,\n",
       " '假肢': 710,\n",
       " '民众': 711,\n",
       " '中正': 712,\n",
       " '对外': 713,\n",
       " '乘坐': 714,\n",
       " '每种': 715,\n",
       " '机构名称': 716,\n",
       " '服务行业': 717,\n",
       " '管理法': 718,\n",
       " '底': 719,\n",
       " '缓释片': 720,\n",
       " '负有': 721,\n",
       " '主观': 722,\n",
       " '件': 723,\n",
       " '福建省': 724,\n",
       " '同济大学': 725,\n",
       " '高新区': 726,\n",
       " '参加': 727,\n",
       " '深圳市人民政府': 728,\n",
       " '第六十六': 729,\n",
       " '分管领导': 730,\n",
       " '转入': 731,\n",
       " '扣留': 732,\n",
       " '孵化': 733,\n",
       " '洛索洛芬': 734,\n",
       " '监测数据': 735,\n",
       " '收缩': 736,\n",
       " '代码': 737,\n",
       " '顺利': 738,\n",
       " '硫卓': 739,\n",
       " '社医监': 740,\n",
       " '数据库': 741,\n",
       " '郑峰': 742,\n",
       " '次选': 743,\n",
       " '人民团体': 744,\n",
       " '促进作用': 745,\n",
       " '披露': 746,\n",
       " '关': 747,\n",
       " '承受': 748,\n",
       " '换药': 749,\n",
       " '血红蛋白': 750,\n",
       " '党内': 751,\n",
       " '听取汇报': 752,\n",
       " '测定仪': 753,\n",
       " '查明': 754,\n",
       " '率先': 755,\n",
       " '市内': 756,\n",
       " '衷心感谢': 757,\n",
       " '不利于': 758,\n",
       " '课题': 759,\n",
       " '竹': 760,\n",
       " '医学专家': 761,\n",
       " '消除': 762,\n",
       " '构语': 763,\n",
       " '审计局': 764,\n",
       " '劳社医': 765,\n",
       " '当月': 766,\n",
       " '头颅': 767,\n",
       " '基质': 768,\n",
       " '摘除术': 769,\n",
       " '金牛': 770,\n",
       " '江人': 771,\n",
       " '机打': 772,\n",
       " '起草文件': 773,\n",
       " '英雄': 774,\n",
       " '省情': 775,\n",
       " '特殊教育': 776,\n",
       " '小点': 777,\n",
       " '杨渡村': 778,\n",
       " '含大隐': 779,\n",
       " '下放': 780,\n",
       " '起步': 781,\n",
       " '工矿': 782,\n",
       " '喉': 783,\n",
       " '栖霞': 784,\n",
       " '到位': 785,\n",
       " '糖浆': 786,\n",
       " '包扎': 787,\n",
       " '片瓶': 788,\n",
       " '微信': 789,\n",
       " '唐氏': 790,\n",
       " '透析液': 791,\n",
       " '岗位责任': 792,\n",
       " '虚拟': 793,\n",
       " '自我保健': 794,\n",
       " '单发': 795,\n",
       " '千家万户': 796,\n",
       " '工本': 797,\n",
       " '尔巴': 798,\n",
       " '合理收费': 799,\n",
       " '扫': 800,\n",
       " '号文': 801,\n",
       " '吸杯': 802,\n",
       " '审方': 803,\n",
       " '巴音郭楞蒙古自治州': 804,\n",
       " '内强': 805,\n",
       " '延长': 806,\n",
       " '组分': 807,\n",
       " '常用': 808,\n",
       " '二十三日': 809,\n",
       " '附近': 810,\n",
       " '滥用职权': 811,\n",
       " '女满': 812,\n",
       " '金为': 813,\n",
       " '放管服': 814,\n",
       " '不满': 815,\n",
       " '房间隔': 816,\n",
       " '仍应': 817,\n",
       " '中央军委': 818,\n",
       " '毫克': 819,\n",
       " '药采': 820,\n",
       " '欠缴': 821,\n",
       " '结核菌': 822,\n",
       " '尿道': 823,\n",
       " '左右': 824,\n",
       " '骨髓瘤': 825,\n",
       " '达沙替': 826,\n",
       " '三色': 827,\n",
       " '实发': 828,\n",
       " '排泄': 829,\n",
       " '定药': 830,\n",
       " '初访': 831,\n",
       " '尿路感染': 832,\n",
       " '七进': 833,\n",
       " '天可': 834,\n",
       " '合法收入': 835,\n",
       " '偏低': 836,\n",
       " '按应': 837,\n",
       " '德阳': 838,\n",
       " '交流': 839,\n",
       " '之上': 840,\n",
       " '升级': 841,\n",
       " '直属机构': 842,\n",
       " '资源共享': 843,\n",
       " '补片': 844,\n",
       " '邹巧': 845,\n",
       " '令': 846,\n",
       " '县外': 847,\n",
       " '免疫治疗': 848,\n",
       " '搭车': 849,\n",
       " '严格遵守': 850,\n",
       " '缺': 851,\n",
       " '少数': 852,\n",
       " '乐业县': 853,\n",
       " '带领': 854,\n",
       " '第五批': 855,\n",
       " '毒素': 856,\n",
       " '专业': 857,\n",
       " '书面': 858,\n",
       " '乘以': 859,\n",
       " '单列': 860,\n",
       " '第二次': 861,\n",
       " '抑酸剂': 862,\n",
       " '市医救': 863,\n",
       " '暂时性': 864,\n",
       " '专业人才': 865,\n",
       " '贾汪区': 866,\n",
       " '城阳区': 867,\n",
       " '着力点': 868,\n",
       " '中医药大学': 869,\n",
       " '当前工作': 870,\n",
       " '视频会议': 871,\n",
       " '行使': 872,\n",
       " '地域': 873,\n",
       " '编写': 874,\n",
       " '不足': 875,\n",
       " '行政监督': 876,\n",
       " '臂': 877,\n",
       " '品种': 878,\n",
       " '颈': 879,\n",
       " '归口': 880,\n",
       " '肌炎': 881,\n",
       " '肝移植': 882,\n",
       " '信息管理': 883,\n",
       " '责任感': 884,\n",
       " '计费': 885,\n",
       " '届满时': 886,\n",
       " '槐荫区': 887,\n",
       " '芷江侗族自治县': 888,\n",
       " '多胞胎': 889,\n",
       " '编委会': 890,\n",
       " '青光眼': 891,\n",
       " '否': 892,\n",
       " '停电': 893,\n",
       " '通力合作': 894,\n",
       " '认真细致': 895,\n",
       " '科教': 896,\n",
       " '页': 897,\n",
       " '中江县': 898,\n",
       " '审计': 899,\n",
       " '十六': 900,\n",
       " '集中式': 901,\n",
       " '谭彦国': 902,\n",
       " '假日': 903,\n",
       " '中兴': 904,\n",
       " '泵次': 905,\n",
       " '机动车辆': 906,\n",
       " '收费': 907,\n",
       " '起始': 908,\n",
       " '随意': 909,\n",
       " '财政部门': 910,\n",
       " '性': 911,\n",
       " '成年': 912,\n",
       " '沟通': 913,\n",
       " '港务': 914,\n",
       " '3.0%': 915,\n",
       " '检查仪': 916,\n",
       " '注意': 917,\n",
       " '江北区': 918,\n",
       " '按季': 919,\n",
       " '淫秽': 920,\n",
       " '星期天': 921,\n",
       " '肝铜': 922,\n",
       " '非尼片': 923,\n",
       " '八钢': 924,\n",
       " '诉讼请求': 925,\n",
       " '尺骨': 926,\n",
       " '消费': 927,\n",
       " '日市': 928,\n",
       " '新居': 929,\n",
       " '并入': 930,\n",
       " '荣誉': 931,\n",
       " '扩张型': 932,\n",
       " '送技': 933,\n",
       " '追加': 934,\n",
       " '器官': 935,\n",
       " '人须': 936,\n",
       " '见': 937,\n",
       " '机场路': 938,\n",
       " '电子信箱': 939,\n",
       " '家园': 940,\n",
       " '以苗': 941,\n",
       " '拨至市': 942,\n",
       " '给予帮助': 943,\n",
       " '滤过': 944,\n",
       " '信息反馈': 945,\n",
       " '中有何': 946,\n",
       " '深入基层': 947,\n",
       " '每根管': 948,\n",
       " '寻查': 949,\n",
       " '境内': 950,\n",
       " '建制': 951,\n",
       " '劳动法': 952,\n",
       " '杨木': 953,\n",
       " '宽严': 954,\n",
       " '局务': 955,\n",
       " '秋': 956,\n",
       " '卫计': 957,\n",
       " '业务人员': 958,\n",
       " '成本核算': 959,\n",
       " '取栓术': 960,\n",
       " '送往': 961,\n",
       " '掌握': 962,\n",
       " '甲下': 963,\n",
       " '斯拉': 964,\n",
       " '相济': 965,\n",
       " '到来': 966,\n",
       " '二维码': 967,\n",
       " '快奖': 968,\n",
       " '肺结核病': 969,\n",
       " '基本保障': 970,\n",
       " '吴方林': 971,\n",
       " '颁发': 972,\n",
       " '酸钠': 973,\n",
       " '一人': 974,\n",
       " '慢性期': 975,\n",
       " '外国': 976,\n",
       " '兴建': 977,\n",
       " '严肃': 978,\n",
       " '艾塞': 979,\n",
       " '组团': 980,\n",
       " '平均价': 981,\n",
       " '阳光': 982,\n",
       " '到期': 983,\n",
       " '枸橼酸': 984,\n",
       " '界定': 985,\n",
       " '践行': 986,\n",
       " '申投': 987,\n",
       " '接待': 988,\n",
       " '问卷': 989,\n",
       " '中不带': 990,\n",
       " '生物制药': 991,\n",
       " '转账': 992,\n",
       " '大类': 993,\n",
       " '欧楷权': 994,\n",
       " '合自': 995,\n",
       " '反腐败': 996,\n",
       " '发明奖': 997,\n",
       " '疾控': 998,\n",
       " '实际行动': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Encoding functions\n",
    "\n",
    "Multihot encoding here relies on 3 functions and uses a memory-efficient datatype (np.unit8).\n",
    "\n",
    "1. The multihot sentence encoder functgion proceeds in several steps:\\\n",
    "Step 1: Create an array of zeros with the specified vocabulary_size\\\n",
    "Step 2: Iterate through each token in the tokenized and filtered sentences: Check if the token is present in the vocabulary dictionary (2a), and if so set the corresponding index in the encoding array to 1 (2b).\\\n",
    "Step 3: Return the multi-hot encoded array.\n",
    "\n",
    "The result is a binary encoding where each position in the array represents the presence or absence of a token in the input tokenized_sen. If a token is present, the corresponding position in the encoding is set to 1; otherwise, it remains 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoding function:\n",
    "def multihot_sentence_encoder(tokenized_sen, vocabulary_size, token_to_index):\n",
    "    encoding = np.zeros(vocabulary_size, dtype=np.uint8) # step 1, non-demanding data type\n",
    "    for token in tokenized_sen: # step 2\n",
    "        if token in token_to_index: # 2a\n",
    "            encoding[token_to_index[token]] = 1 # 2b\n",
    "    return encoding # step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The function encode_batch applies multihot encoding to a batch of sentences, rather than the whole dataset at a time. It returns a sparse matrix (CSR format) with the encoded batch, rather than a regular numpy array. Both steps help limiting the computational resources required to process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def encode_batch(sentences, vocabulary_size, token_to_index):\n",
    "    encoded_batch = [multihot_sentence_encoder(sen, vocabulary_size, token_to_index) for sen in sentences]\n",
    "    return csr_matrix(encoded_batch, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The function process_in_batches can be applied to an entire data frame. The argument batch_size determines how many sentences to process at a time, it can be adjusted to the computational resources available. It returs a sparse matrix representing the entire dataset with multihot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_in_batches(sentences, batch_size, vocabulary_size, token_to_index):\n",
    "    all_encoded_data = []\n",
    "    for start_idx in range(0, len(sentences), batch_size):\n",
    "        batch_sentences = sentences[start_idx:start_idx+batch_size]\n",
    "        encoded_batch = encode_batch(batch_sentences, vocabulary_size, token_to_index)\n",
    "        all_encoded_data.append(encoded_batch)\n",
    "        \n",
    "    # Concatenate all batches and create to sparse matrix\n",
    "    return vstack(all_encoded_data).astype(np.uint8) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Encode the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, encode the training data, save it as a sparse matrix, and then recover the data and load it with the vocabulary as column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'uint8'\n",
       "\twith 251730 stored elements and shape (13576, 18559)>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process the training data in batches\n",
    "X_multihot_train_sparse = process_in_batches(X_toksen_train['tokenized_sen_filtered'], \n",
    "                                         1000, # batch size\n",
    "                                         len(vocabulary), token_to_index)\n",
    "X_multihot_train_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./y_broad/X_multihot_train_sparse.pkl']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save sparse matrix using joblib\n",
    "joblib.dump(X_multihot_train_sparse, './y_broad/X_multihot_train_sparse.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "loaded_X_multihot_train_sparse = joblib.load('./y_broad/X_multihot_train_sparse.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>入市</th>\n",
       "      <th>助学金</th>\n",
       "      <th>停保</th>\n",
       "      <th>征询</th>\n",
       "      <th>二十五年</th>\n",
       "      <th>党校</th>\n",
       "      <th>及下</th>\n",
       "      <th>送市</th>\n",
       "      <th>氢溴酸</th>\n",
       "      <th>中轴</th>\n",
       "      <th>...</th>\n",
       "      <th>人性化</th>\n",
       "      <th>参合</th>\n",
       "      <th>法制观念</th>\n",
       "      <th>拔牙</th>\n",
       "      <th>元春</th>\n",
       "      <th>庞秀萍</th>\n",
       "      <th>1.3%</th>\n",
       "      <th>托</th>\n",
       "      <th>栏须</th>\n",
       "      <th>八村</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13571</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13572</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13573</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13574</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13575</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13576 rows × 18559 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       入市  助学金  停保  征询  二十五年  党校  及下  送市  氢溴酸  中轴  ...  人性化  参合  法制观念  拔牙  元春  \\\n",
       "0       0    0   0   0     0   0   0   0    0   0  ...    0   0     0   0   0   \n",
       "1       0    0   0   0     0   0   0   0    0   0  ...    0   1     0   0   0   \n",
       "2       0    0   0   0     0   0   0   0    0   0  ...    0   0     0   0   0   \n",
       "3       0    0   0   0     0   0   0   0    0   0  ...    0   0     0   0   0   \n",
       "4       0    0   0   0     0   0   0   0    0   0  ...    0   0     0   0   0   \n",
       "...    ..  ...  ..  ..   ...  ..  ..  ..  ...  ..  ...  ...  ..   ...  ..  ..   \n",
       "13571   0    0   0   0     0   0   0   0    0   0  ...    0   0     0   0   0   \n",
       "13572   0    0   0   0     0   0   0   0    0   0  ...    0   0     0   0   0   \n",
       "13573   0    0   0   0     0   0   0   0    0   0  ...    0   0     0   0   0   \n",
       "13574   0    0   0   0     0   0   0   0    0   0  ...    0   0     0   0   0   \n",
       "13575   0    0   0   0     0   0   0   0    0   0  ...    0   0     0   0   0   \n",
       "\n",
       "       庞秀萍  1.3%  托  栏须  八村  \n",
       "0        0     0  0   0   0  \n",
       "1        0     0  0   0   0  \n",
       "2        0     0  0   0   0  \n",
       "3        0     0  0   0   0  \n",
       "4        0     0  0   0   0  \n",
       "...    ...   ... ..  ..  ..  \n",
       "13571    0     0  0   0   0  \n",
       "13572    0     0  0   0   0  \n",
       "13573    0     0  0   0   0  \n",
       "13574    0     0  0   0   0  \n",
       "13575    0     0  0   0   0  \n",
       "\n",
       "[13576 rows x 18559 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame with the vocabulary as column names\n",
    "X_multihot_train = pd.DataFrame(loaded_X_multihot_train_sparse.toarray(), columns=vocabulary)\n",
    "X_multihot_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, process the test data. Recovery works as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'uint8'\n",
       "\twith 120535 stored elements and shape (6688, 18559)>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process the training data in batches\n",
    "X_multihot_test_sparse = process_in_batches(X_toksen_test['tokenized_sen_filtered'], \n",
    "                                         1000, # batch size\n",
    "                                         len(vocabulary), token_to_index)\n",
    "X_multihot_test_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./y_broad/X_multihot_test_sparse.pkl']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save sparse matrix using joblib\n",
    "joblib.dump(X_multihot_test_sparse, './y_broad/X_multihot_test_sparse.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, process the unlabelled data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data in batches\n",
    "X_unlabelled_multihot_sparse = process_in_batches(data_unlabelled['tokenized_sen_filtered'], \n",
    "                                         1000, # batch size\n",
    "                                         len(vocabulary), token_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./X_unlabelled_multihot_sparse.pkl']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save sparse matrix using joblib\n",
    "joblib.dump(X_unlabelled_multihot_sparse, './X_unlabelled_multihot_sparse.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to convert the sparse matrix into a regular dataframe with the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try to load the data with feature names\n",
    "X_unlabelled_multihot = pd.DataFrame(X_unlabelled_multihot_sparse.toarray(), columns=vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Visual inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visual inspection \n",
    "You can reverse the encoding process to see if you can reconstruct the original sentence from the generated multi-hot encoding. However, this might not always be straightforward due to the loss of sequence information in multi-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>入市</th>\n",
       "      <th>助学金</th>\n",
       "      <th>停保</th>\n",
       "      <th>征询</th>\n",
       "      <th>二十五年</th>\n",
       "      <th>党校</th>\n",
       "      <th>及下</th>\n",
       "      <th>送市</th>\n",
       "      <th>氢溴酸</th>\n",
       "      <th>中轴</th>\n",
       "      <th>...</th>\n",
       "      <th>人性化</th>\n",
       "      <th>参合</th>\n",
       "      <th>法制观念</th>\n",
       "      <th>拔牙</th>\n",
       "      <th>元春</th>\n",
       "      <th>庞秀萍</th>\n",
       "      <th>1.3%</th>\n",
       "      <th>托</th>\n",
       "      <th>栏须</th>\n",
       "      <th>八村</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13571</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13572</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13573</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13574</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13575</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13576 rows × 18559 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        入市  助学金   停保   征询  二十五年   党校   及下   送市  氢溴酸   中轴  ...  人性化   参合  法制观念  \\\n",
       "0      0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   0.0   \n",
       "1      0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   0.0   \n",
       "2      0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   0.0   \n",
       "3      0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   0.0   \n",
       "4      0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   0.0   \n",
       "...    ...  ...  ...  ...   ...  ...  ...  ...  ...  ...  ...  ...  ...   ...   \n",
       "13571  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   0.0   \n",
       "13572  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   0.0   \n",
       "13573  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   0.0   \n",
       "13574  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   0.0   \n",
       "13575  0.0  0.0  0.0  0.0   0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0   0.0   \n",
       "\n",
       "        拔牙   元春  庞秀萍  1.3%    托   栏须   八村  \n",
       "0      0.0  0.0  0.0   0.0  0.0  0.0  0.0  \n",
       "1      0.0  0.0  0.0   0.0  0.0  0.0  0.0  \n",
       "2      0.0  0.0  0.0   0.0  0.0  0.0  0.0  \n",
       "3      0.0  0.0  0.0   0.0  0.0  0.0  0.0  \n",
       "4      0.0  0.0  0.0   0.0  0.0  0.0  0.0  \n",
       "...    ...  ...  ...   ...  ...  ...  ...  \n",
       "13571  0.0  0.0  0.0   0.0  0.0  0.0  0.0  \n",
       "13572  0.0  0.0  0.0   0.0  0.0  0.0  0.0  \n",
       "13573  0.0  0.0  0.0   0.0  0.0  0.0  0.0  \n",
       "13574  0.0  0.0  0.0   0.0  0.0  0.0  0.0  \n",
       "13575  0.0  0.0  0.0   0.0  0.0  0.0  0.0  \n",
       "\n",
       "[13576 rows x 18559 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "loaded_X_multihot_train_sparse = joblib.load('./y_broad/X_multihot_train_sparse.pkl')\n",
    "# Create a DataFrame with the vocabulary as column names\n",
    "X_multihot_train = pd.DataFrame(loaded_X_multihot_train_sparse.toarray(), columns=vocabulary)\n",
    "X_multihot_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dense DataFrame: (13576, 18559)\n",
      "Vocabulary size: 18559\n",
      "Shape of the sparse matrix: (13576, 18559)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of the dense DataFrame: {X_multihot_train.shape}\")\n",
    "print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "print(f\"Shape of the sparse matrix: {loaded_X_multihot_train_sparse.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct sentences from the dense DataFrame\n",
    "reconstructed_sentences = []\n",
    "\n",
    "for _, row in X_multihot_train.iterrows():\n",
    "    reconstructed = row[row == 1].index.tolist() # Select columns where the value is 1\n",
    "    reconstructed_sentences.append(reconstructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['椎弓', '金牛', '肝铜', '一系列', '开单', '热情']\n"
     ]
    }
   ],
   "source": [
    "# Print reconstructed sentences\n",
    "print(reconstructed_sentences[154])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['健全', '统一', '规范', '医疗', '救助', '制度']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_toksen_train['tokenized_sen_filtered'][154]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. TF-IDF encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Corpus creation\n",
    "\n",
    "To generate TF-IDF (Term Frequency-Inverse-Document-Frequency) values from Chinese tokenized sentences stored as a column of lists in a Pandas DataFrame, we use the TfidfVectorizer class from the sklearn.\n",
    "\n",
    "First, we create a corpus of space-separated tokens from the tokenized and filtered sentences in list form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert tokenized sentences to space-separated strings (training data)\n",
    "corpus_train = [' '.join(sentence) for sentence in X_toksen_train['tokenized_sen_filtered']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(corpus_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13576, 13576)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check dimensions\n",
    "len(corpus_train), len(X_toksen_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['第二十八条 各级 人力 社保 行政部门 应 会同 部门 积极 推进 居民 医保 结算 制度 改革 实施 总额 控制 多种 结算 方式 有效 控制 医疗 费用 过快 增长 减轻 参保 人员 负担',\n",
       " '建立 动态 调整 高效 运行 多元 复合式 医保 支付 体系 充分发挥 基本 医疗保险 激励 约束 控制 医疗 费用 不合理 增长 作用',\n",
       " '做好 网站 安全 管理 维护 省 医保 局 委托 山西 云 时代 有限公司 为局 网站 安全 管理 提供 技术 人员 硬件 设备 保障',\n",
       " '已婚 女职工 放置 取出 节育环 医疗费 元 皮下 埋置 医疗费 元 结扎 医疗费 元 已婚 男 职工 结扎 医疗费 元',\n",
       " '银行 柜台 缴费 持 身份证 医保 本到 市县 工商银行 建设银行 农业银行 中国银行 交通银行 邮储 银行 广西 农信社 柳州 银行 桂林 银行 光大银行 兴业银行 北部湾 银行 家 签约 银行 储蓄 网点 柜台 缴纳 保费 利用 银行 提供 自助 缴费 线上 缴费 缴费 方式 缴纳 保费',\n",
       " '工作 要求 各级 新农 合 主管部门 高度重视 切实 落实 农村居民 重大 疾病 医疗保障 工作 深化 医改 重要 举措 积极争取 政府 部门 重大 疾病 医疗保障 工作 重视 支持',\n",
       " '保障 范围',\n",
       " '清洁 照料 会阴 清洁 次日 护理 对象 清洁 习惯 原则 次 会阴部 有无 伤口 有无 大小便 失禁 留置 尿管 帮助 护理 对象 完成 会阴部 擦洗 冲洗 水温 适宜 动作 轻柔 保护 隐私',\n",
       " '诊疗 设备 医用 材料 类 应用 眼科 准分子 激光 治疗仪 糖尿病 决策支持系统 人体 信息 诊断 电脑 选择 最佳 妊娠期 检查 治疗 费用 眼镜 义齿 义眼 义肢 助听器 康复 性器具 自用 保健 按摩 检查和 治疗 器械 按摩器 轮椅 拐杖 家用 检测 治疗 仪器 皮钢 背甲 腰围 钢 头颈 胃托 肾托 子宫 托 疝气 带 护膝 带 提睾带 健脑 器 药枕 药垫 热敷 袋 神功 元气 袋 费用 省 市 物价部门 规定 不可 单独 收费 一次性 医用 材料',\n",
       " '第二条 持有 本市 常住 户籍 符合 办法 规定 救助 条件 困难 居民 均 当地政府 获得 医疗 救助',\n",
       " '办公室 决策 执行 部门 落实 情况 实施 督查',\n",
       " '实行 一体化 管理 基层 医疗卫生 机构 开展 门诊 报销 方便 参合 人员 就近 就医',\n",
       " '测试 成熟 智能 审核 规则 及时 植入 全省 医保 信息 平台 智能 审核 功能模块 违规 收费 典型 问题 实施 实时 动态 智能 监审',\n",
       " '资金 基金 保障 费用 个人账户 支付 费用 应由 第三 负担 费用 国家 省 规定 不予 支付 费用',\n",
       " '城镇居民 医疗保险 基金 新农 合 基金 合并 纳入 财政 专户 管理 人力资源 社会保障 部门 支付',\n",
       " '续保 参保 人员 待遇 期满 办理 年度 续保 缴费 直接 接续 年度 享受 待遇',\n",
       " '第三十七条 参保 定点 医疗机构 使用 个人帐户 余额 就医',\n",
       " '标准 按本 企业 应 参加 合作医疗 职工 人数 上年 计税 工资 4% 缴费 委托 税务 部门 乡镇 征集',\n",
       " '附件 荆门市 基本 医疗保险 基金 统收统支 管理 暂行办法 第一章 总则 第一条 规范 我市 基本 医疗保险 基金 统收统支 核拨 管理工作 省 人民政府 办公厅 全面 做实 基本 医疗保险 市级 统筹 实施 意见 鄂 政办发 号 精神 结合 我市 实际 制定 办法',\n",
       " '含术 线 引导',\n",
       " '发生 医疗 费用 查证 核实 不同 医疗机构 级别 报帐',\n",
       " '第四章 大病 保险 承办 方式 第八条 城乡居民 大病 保险 以市 投保 单位 由市 人力资源 社会 保障局 牵头 公开招标 比选 方式 市内 确定 具有 开展 大病 保险业务 资质 商业保险 机构 承办 全市 城乡居民 大病 保险业务 由市 医疗保险 经办 机构 统一 全市 参保 城乡居民 投保',\n",
       " '心电图 检查 动态 心电图 心电图 负荷 试验',\n",
       " '财政部门 负责 职工基本 医疗保险 基金 管理 基金 使用 情况 进行 监督 负责 单位 缴费 财政负担 部分 足额 列入 预算 及时 拨付',\n",
       " '综上 目前 我省 医疗保障 基金 监管 体系 健全 监管 执法 经办 职责 定位 清晰 链条 完整 执法 主体 明确 贯彻落实 中央 改革 要求 符合 国办发 号 文件精神',\n",
       " '专项 借款 偿还 医疗保险 专项 借款 政府 企业 有偿 政策性 借款 借款 企业 应 努力 改善 经营 状况 经营 状况 好转 积极 予以 偿还',\n",
       " '办事 群众 办事 单位 相互 转告 带来 不便 敬请 谅解 详情请 咨询服务 热线 关注 部门 官网 微信 公众 号 查询',\n",
       " '四十 海南省 城镇 从业人员 基本 医疗保险 条例 条款 社会保障 行政部门 修改 社会保险 行政部门 征收 机关 修改 社会保险费 征收 机构 个人帐户 资金 个人帐户 资金 余额 修改 个人帐户 余额',\n",
       " '符合规定 扣',\n",
       " '职工 因公 出差 准假 外出 期间 急诊 外地 医疗机构 住院 需在 天 内向 所在单位 报告 所在单位 省级 医疗保险 服务中心 办理 外诊 登记手续',\n",
       " '加强 政策措施 运行 情况 评估 健全 政策 出台 落实 评估 改进 闭环 机制 完善 全 链条 管理 政策措施 直达 基层 直接 惠及 市场主体 疏堵 消障 加力 提效',\n",
       " '公立 医疗机构 应 本省 自治区 直辖市 医药 集中 采购 平台 采购 需新冠 抗原 检测 试剂 线下 采购',\n",
       " '卫生 健康 部门 监督 指导 定点 医疗机构 开展 规范 诊疗 广西壮族自治区人民政府 办公厅 印发 广西 地中海 贫血 防治 三年 行动计划 通知 桂 政办发 号 要求 进一步 健全 地贫 患者 健康 信息管理 机制 地贫 患者 信息 档案 及时 录入 地贫 患者 信息 数据库 确保 享受 医疗 救助 待遇 地贫 患者 及时 纳入 数据库 管理',\n",
       " '集中 缴费 期 参保 缴费 需 参加 城乡居民 医保 各类 人员 按规定 标准 缴费 完成 参保 登记 待遇 享受 期自 缴费 之日 次 算',\n",
       " '医疗机构 应 积极 配合 采购 配送 监测 疗效 评估 及时 上报 中选 产品 采购 配送 使用 出现 问题',\n",
       " '新农 合 定点 医疗机构 实行 日常 监督 年度 考核 相结合 新农 合 管理机构 每季度 辖区 定点 医疗机构 进行 一次 督查 市级 一年 两次 督查 市 县 新农 合 管理机构 每年 新农 合 定点 医疗机构 进行 一次 综合 考评 县 区级 综合 考评 全 覆盖 市级 综合 考评 抽查 日常 监督 基金 拨付 挂钩 具体办法 见 市卫 新农 合 号 文件',\n",
       " '各级 政府 应 切实加强 领导 精心组织 认真 实施',\n",
       " '制定 规范 移交 程序 妥善处理 体制 制度 并轨 期间 问题 确保 整合 工作 期间 思想 乱 队伍 稳定 工作 不断 资料 丢 基金 国有资产 流失 参保 人员 医疗 待遇 受 影响',\n",
       " '附件 肝 豆状 核 变性 八种 门诊 规定 病种 鉴定 标准 淮南市 人力资源 社会 保障局 附件 肝 豆状 核 变性 八种 门诊 规定 病种 鉴定 标准 肝 豆状 核 变性 缓慢 进行性 震颤 肌 僵直 构语 障碍 椎体 外系 症状 体征 肝 症状 由眼 裂隙 灯下 证实 特异 角膜 色素 环 血清 铜 蓝 蛋白 铜 氧化 尿 铜 ; 扫描 双侧 大脑 豆状 核 低密度 阴影 尾状核 脑 皮质 萎缩 检查 基底节 长 信号 B超 具有 闪烁 岩层 征 树枝状 光带 结节 型 特殊 图 线 96% 骨关节炎 骨质 疏松 骨 软化 常见于 腕关节 以下 后期 出现 肝功能 异常 青霉 胺尿排 铜 负荷 试验 阳性 肝铜 干重 血清 铜 下降 正常值 基因 诊断',\n",
       " '第三十三条 参保 人员 下列 情形 发生 医疗 费用 医疗保险 基金 不予 报销 国外 港澳台地区 治疗 自杀 自残 精神病 患者 进行 打架 斗殴 酗酒 吸毒 违法犯罪 行为 所致 伤病 交通事故 意外 伤害 医疗事故 法律 规定 他方 承担 医疗 费用 责任 美容 矫形 生理 缺陷 进行 治疗 婚检 费用 公共卫生 服务 费用 国家 财政 给予 专项资金 补助 艾滋病 血吸虫病 结核病 病种 医疗 费用 规定 不予 报销 其他费用',\n",
       " '全面 建立 申请 救助 机制 户 申请 村 社区 评议 乡镇 街道 审核 县级 医保 民政 乡村 振兴 部门 联合 确定 程序 因病 致贫 重 病患者 身份 认定 前 当年 个人 自付 合规 医疗 费用 给予 相应 救助',\n",
       " '产 前 门诊 检查 相关 医疗 费用 统筹 基金 50% 标准 支付 统筹 基金 支付 每人 每孕次 元',\n",
       " '第十三条 服役 服刑 结束 判处 管制 宣告 缓刑 假释 暂予 监外执行 社区 矫正 对象 户籍 新 迁入 我市 人员 应 日内 办理 居民 医保 参保 缴费 转移 接续 手续 日内 办理 城乡居民 医保 转移 接续 业务 办理 参保 缴费 业务 期间 发生 符合 基本 医疗保险 规定 医疗费 补录 方式 予以 支付',\n",
       " '年度 个人 缴费 标准 每人每年 元',\n",
       " '新 入学 大学生 办理 年度 参保 缴费 手续 开学 之日起 月底 发生 住院 符合规定 医疗 费用 次年 日后 持 相关 凭证 经办 机构 按规定 报销',\n",
       " '第二十八条 办法 日起 施行',\n",
       " '做好 医疗保障 展示 我市 医疗卫生 事业 发展 难得 机遇',\n",
       " '线下 开展 项目 增加 互联网 服务 方式 医疗机构 就此 提出 新增 项目 申请 优先 线下 开展 项目 增加 计价 说明 处理',\n",
       " '医用 垃圾 污水处理',\n",
       " '发文 办理 涉密 公文 机要 人员 接收 文件 → 局 办公室 主任 提出 拟 办理 意见 → 局 领导 阅批 → 直接 归档 无需 办理 机要文件 办理 意见 分送 相关 处室 单位 → 主办 会办 处室 单位 办理 → 局 办公室 核稿 人员 初核 → 局 办公室 主任 复核 → 局 领导 审签 → 收发 人员 印制 公文 不需 对外 行文 办结 件 处理 → 归档 需 办理 非密 公文 收发 人员 接收 文件 ﹝ → 收文 人员 扫描 纸质 公文 电子 公文 传输 平台 下载 电子 公文 上传 局机关 系统 ﹞ → 局 办公室 副 主任 提出 拟分 办 意见 → 局 领导 阅批 → 主办 会办 处室 单位 办理 → 局 办公室 核稿 人员 初核 → 局 办公室 主任 复核 → 局 领导 审签 → 收发 人员 印制 公文 不需 对外 行文 办结 件 处理 → 归档 传阅 公文 上级 机关 系统 转阅 公文 收发 人员 接收 文件 → 局 办公室 副 主任 提出 传阅 意见 → 相关 人员 阅签 纸质 公文 扫描 到局 系统 文件 收发 人员 接收 文件 → 扫描 公文 局机关 系统 → 局 办公室 副 主任 提出 传阅 意见 → 相关 人员 阅签 → 收发 人员 归档 不宜 扫描 到局 系统 文件 收发 人员 接收 文件 → 局 办公室 副 主任 提出 传阅 意见 → 相关 人员 阅签 → 收发 人员 归档 纸质 传阅 文件 应 及时 阅看 每个 处室 应 收到 传阅 文件 工作日内 阅看 完毕 流转 未阅 处室',\n",
       " '大病 保险 不设 支付 限额',\n",
       " '报销 需 资料 城镇居民 医保 新农 合 规定 执行',\n",
       " '新生儿 户籍地 监护人 居住地 街道 乡镇 办理 参保 缴费 手续 符合 补办 参保 规定 人员 市 县 市 医保 经办 机构 办理 参保 缴费 手续',\n",
       " '体检 地点 南昌大学 第二 附属 医院 地址 : 东湖 院区 南昌市 民德 路 号 红角 洲 院区 海玥 荟旁',\n",
       " '粘弹 剂次 睫状体 脉络膜 上腔放 液术 特殊 缝线 粘弹 剂次 睫状体 特殊 治疗 粘弹剂 睫状体 光凝法 治疗 单侧 睫状体 冷凝 法 治疗 单侧 睫状体 透热 法 治疗 单侧 前 房角 切开术 粘弹 剂次 前 房角 切开术 使用 特殊 仪器 加收 前 房角 镜 次 前房 积血 清除 术 粘弹 剂次 前房 积血 清除 术 使用 特殊 仪器 加收 前 房角 镜 次 房角 粘连 分离 术 粘弹 剂次 房角 粘连 分离 术 使用 特殊 仪器 加收 前 房角 镜 次 前房 成形术 粘弹 剂次 青光眼 滤过 术 含小梁 切除 虹膜 嵌顿 巩膜 灼滤',\n",
       " '基础 病组 自治区 统一 选择',\n",
       " '第四十三条 参保 居民 下列 行为 追回 支付 医疗 费用 外 视其 情节 轻重 给予 批评 教育',\n",
       " '准予 广州市 东山区 妇幼保健 院 名称 变更 广州市 越秀区 妇幼保健 院 分院 法定代表 变更 沈有 高',\n",
       " '巩固 拓展 脱贫 攻坚 成果 有效 衔接 乡村 振兴 战略 自治区 四大 提升 行动 统筹 推进 推动 医疗保障 制度 持续 发展',\n",
       " '适时 组织 赴 监管 方式 创新 先进 省市 学习 取经 对照 标杆 完善 提升 提高 创新 试点工作 成效',\n",
       " '干部职工 行政 相对 人及 社会公众 金辛 医药 服务处 医保 中心 事务 中心',\n",
       " '第二十一条 定点 医疗机构 须 医疗保险 经办 机构 要求 及时 做好 本院 信息系统 医疗保险 信息系统 对接 工作 须 及时 规范 完整 准确 地向 医疗保险 信息 网络系统 上传 参保 职工 患者 就医 信息',\n",
       " '十九 上级 政府 如有 新 政策 调整 由市 劳动 保障局 会同 市 财政局 对本 实施 意见 相关 内容 进行 调整 补充',\n",
       " '第八章 附则 第三十二条 办法 日起 施行 有效期',\n",
       " '第十二条 定点 医疗机构 年度 总额 控制 指标 确定 平均 月度 控制 指标 定点 医疗机构 每月 平均 控制 指标 系统 支撑 每月 实际 发生额 申请 拨付 医疗 费用',\n",
       " '及时 掌握 职工 医疗保险 基金 征收 进度 地税 财政部门 应 及时 金库 帐 按时 通报 收入 进度',\n",
       " '征收 工作 结束 组织 考核 评比 完成 计划 任务 100% 单位 由市 农 合办 向市 财政 提出申请 日前 该镇 征收 金额 1% 给予 奖励 先进 单位 积极分子 进行 表彰 评比 合格 单位 通报批评',\n",
       " '施行 日期 办法 日起 执行',\n",
       " '灵活 就业 人员 参加 基本 医疗保险 实行 最低 缴费 年限 制度 实际 缴费 年限 制度',\n",
       " '县 财政 应 每年 日前 县 补助 资金 划拨 县 确定 金融机构 基金 专用 帐户 县 合管 办于 每年 日前 农民 个人 存储 费 参加 合作医疗 人数 数据资料 金融机构 验资 证明 报市 财政 市 卫生部门 落实 合作医疗 配套 补助 资金',\n",
       " '全国 医疗保障 信息系统 建设 试点工作 探索 省级 集中 模式 统一 开展 智能 监控 工作 不断 提升 监控 效能',\n",
       " '第一条 办法 适用 参加 基本 医疗保险 单位 保健 对象 离休 人员 二等 乙级 革命 残废军人 下同 未 参保 单位 保健 对象 适用 办法',\n",
       " '第七条 自治区 医保 局 办公室 国家 医保 局 工作 重点 编制 自治区 医保 局 年度 新闻宣传 工作 要点 组织 实施',\n",
       " '社会保险 为主 商业保险 为辅 原则 做好 参加 商业保险 大学生 参加 城镇居民 医疗保险 衔接 工作',\n",
       " '同意 提高 门诊 统筹 补偿 比例 定点 乡镇 卫生院 定点 村 卫生室 门诊 统筹 补偿 比例 原来 30% 25% 一律 提高 40%',\n",
       " '健全 督导 问责 机制 组织 部门 开展 联合 督查 各地 执行 坚决 彻底 到位 督促 纠正',\n",
       " '新 参合 人员 缴费 一并 上交 有效证件 复印件 乡镇 合作医疗 管理所 统一 办理 IC卡',\n",
       " '以收定 支 收支平衡 略有结余 总额 预算编制 机制 深化 住院 门诊 药品 耗材 医疗 服务 统筹 地区 就医 转外 就医 之间 分项 预算 机制',\n",
       " '全力 确保 农村 贫困人口 应保 尽保 巩固 维护 贫困人口 动态 应保 尽保 局面',\n",
       " '四十八条 经办 机构 发现 定点 医疗机构 违约 行为 应当 及时 协议 处理',\n",
       " '调整 重大 疾病 救助 基金 缴费 标准 参加 基本 医疗保险 人员 重大 疾病 医疗 救助 基金 缴费 标准 原来 上年 社平 工资 0.6% 缴纳 统一 调整 每人 每月 元 标准 缴纳',\n",
       " '连锁 医药公司 下属 零售 药店 具有 医保 定点 零售 药店 资格 数量 得分 家 家 以下 家 家 以下 家 家 以下 家',\n",
       " '基本 医疗保险 定点 医疗机构 审定 以下 办法 程序 进行 愿意 能力 承担 基本 医疗保险 定点 医疗 服务 医疗机构 可向市 劳动保障部 门 提出申请 按规定 要求 提供 相关 材料 市 劳动保障部 门 医疗机构 申请 定点 提供 材料 进行 审查 材料 齐全 具备 定点 医疗机构 资格 条件 项 提交 评审 委员会 评审 委员会 是否 符合 定点 医疗机构 条件 项 要求 进行 现场 检查 评审',\n",
       " '第十三条 职工 分娩 终止 妊娠 施行 计划生育 手术 住院 期间 诊治 妊娠 合并症 并发症 需要 规定 转 省内 职工基本 医疗保险 定点 医疗机构 就医 需 医疗 费用 医疗保障 经办 机构 医疗机构 规定 直接 结算',\n",
       " '病种 恶性肿瘤 器官移植 后抗 排斥 治疗 慢性 肾功能 尿毒症 期 帕金森氏 综合症 严重 精神病 系统性 红斑狼疮 再生 障碍性 贫血',\n",
       " '本次 下达 补助 资金 更改 城乡居民 医疗保险 资金 拨付 方式 通知 粤 财社 号 采取 直接 支付 方式 由省 财政 直接 支付 各市 财政 专户',\n",
       " '医疗 证 卡 借给 就诊 开 虚假 医药费 收据 处方 冒领 城镇居民 医疗保险 补助 资金 原因 遵守 城镇居民 医疗保险 规定 造成 医疗 费用 不能 正常 支付 无理取闹 私自 涂改 医药费 收据 病历 处方 检查报告 利用 城镇居民 医疗保险 定点 医疗机构 开出 药品 进行 非法 倒卖 违反 城镇居民 医疗保险 管理 规定 行为',\n",
       " '补偿 比例 特殊 病种 门诊 费用 起付线 费用 符合 补偿 范围 费用 30% 补偿 精神病 肺结核 40% 补偿',\n",
       " '〇 〇 九年 五月 十三日',\n",
       " '按规定 处理 废物 接受 临床 相关 咨询']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_train[10:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test corpus\n",
    "corpus_test = [' '.join(sentence) for sentence in X_toksen_test['tokenized_sen_filtered']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6688, 6688)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check dimensions\n",
    "len(corpus_test), len(X_toksen_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelled corpus\n",
    "corpus_unlabelled = [' '.join(sentence) for sentence in data_unlabelled['tokenized_sen_filtered']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(993526, 993526)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check dimensions\n",
    "len(corpus_unlabelled), len(data_unlabelled['tokenized_sen_filtered'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Transform corpus to TF-IDF vectors\n",
    "\n",
    "Train the vectorizer on the training data, then vectorize all data sets with that vectorizer to ensure consistent vocabulary. To obtain a manageable number of features, only unigrams and bigrams are considered that appear (1) in at least 3 sentences, and (b) less than 80% of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), # unigrams and bigrams\n",
    "                                   min_df=3, # appears at least in 3 documents \n",
    "                                   max_df=0.8, # appears in at most 80% of the documents\n",
    "                                   dtype = np.float32) # less demanding for memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train TF-IDF vectorizer on the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float32'\n",
       "\twith 351821 stored elements and shape (13576, 21960)>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit and transform the corpus to TF-IDF vectors\n",
    "tfidf_matrix_train = tfidf_vectorizer.fit_transform(corpus_train)\n",
    "tfidf_matrix_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['10' '10 二级' '10 以内' ... '龙岩' '龙岩市' '龙岩市 人民政府']\n",
      "TF-IDF Matrix:\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Check vocabulary and TF-IDF scores\n",
    "print(\"Vocabulary:\", tfidf_vectorizer.get_feature_names_out())\n",
    "print(\"TF-IDF Matrix:\\n\", tfidf_matrix_train.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21960"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many features were created?\n",
    "len(tfidf_vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the feature names\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "type(tfidf_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the feature names\n",
    "pd.DataFrame(tfidf_feature_names).to_csv('tfidf_feature_names.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21960"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recover features, get column and turn to list\n",
    "tfidf_feature_names = pd.read_csv('tfidf_feature_names.csv').iloc[:,1].tolist()\n",
    "len(tfidf_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./y_broad/tfidf_matrix_train.pkl']"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the data as a sparse matrix\n",
    "joblib.dump(tfidf_matrix_train, './y_broad/tfidf_matrix_train.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and create a data frame with feature names\n",
    "loaded_X_tfidf_train_sparse = joblib.load('./y_broad/tfidf_matrix_train.pkl')\n",
    "# Create a DataFrame with the vocabulary as column names\n",
    "#X_tfidf_train = pd.DataFrame(loaded_X_tfidf_train_sparse.toarray(), columns=tfidf_feature_names)\n",
    "#X_tfidf_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float32'\n",
       "\twith 164329 stored elements and shape (6688, 21960)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the test data using the same vectorizer\n",
    "tfidf_matrix_test = tfidf_vectorizer.transform(corpus_test)\n",
    "tfidf_matrix_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./y_broad/tfidf_matrix_test.pkl']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the data\n",
    "joblib.dump(tfidf_matrix_test, './y_broad/tfidf_matrix_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorize the unlabelled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Compressed Sparse Row sparse matrix of dtype 'float32'\n",
       "\twith 24295333 stored elements and shape (993526, 21960)>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the unlabelled data using the same vectorizer\n",
    "tfidf_matrix_unlabelled = tfidf_vectorizer.transform(corpus_unlabelled)\n",
    "tfidf_matrix_unlabelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./tfidf_matrix_unlabelled_broadvoc.pkl']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the data\n",
    "joblib.dump(tfidf_matrix_unlabelled, './tfidf_matrix_unlabelled_broadvoc.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the unlabelled data, we might encounter memory problems ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 81.3 GiB for an array with shape (993526, 21960) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_tfidf_unlabelled \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\u001b[43mtfidf_matrix_unlabelled\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, columns\u001b[38;5;241m=\u001b[39mtfidf_feature_names)\n",
      "File \u001b[1;32mc:\\Users\\Paul\\anaconda3\\envs\\ConPy310\\lib\\site-packages\\scipy\\sparse\\_compressed.py:1170\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1169\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1170\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_toarray_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1172\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Paul\\anaconda3\\envs\\ConPy310\\lib\\site-packages\\scipy\\sparse\\_base.py:1366\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1366\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 81.3 GiB for an array with shape (993526, 21960) and data type float32"
     ]
    }
   ],
   "source": [
    "# try to load the data with feature names\n",
    "X_tfidf_unlabelled = pd.DataFrame(tfidf_matrix_unlabelled.toarray(), columns=tfidf_feature_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ConPy310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
